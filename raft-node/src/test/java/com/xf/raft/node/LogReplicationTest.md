# Raft 日志复制功能测试用例设计

## 一、基础日志复制测试

### 1. 单条日志复制
**测试目标**：验证 Leader 能成功将单条日志复制到所有 Follower
- 启动 3 节点集群，等待选出 Leader
- 通过客户端向 Leader 发送 1 条 PUT 请求
- 验证：
    - Leader 返回成功
    - 所有 Follower 都收到该日志（检查 lastIndex）
    - 所有节点的状态机都应用了该日志（GET 能读到）
    - commitIndex 和 lastApplied 正确更新

### 2. 批量日志复制
**测试目标**：验证连续写入多条日志的正确性
- 启动 5 节点集群
- 连续发送 100 条 PUT 请求（不同的 key-value）
- 验证：
    - 所有请求都返回成功
    - 所有节点的日志数量一致（lastIndex = 100）
    - 所有节点的状态机数据一致（随机 GET 10 个 key）
    - 日志的顺序在所有节点上一致

### 3. 并发写入测试
**测试目标**：多个客户端并发写入
- 启动 3 节点集群
- 启动 5 个并发客户端线程，每个写入 20 条数据
- 验证：
    - 所有写入最终都成功（可能有重试）
    - 总共 100 条日志，无丢失
    - 所有节点状态机数据一致
    - 没有出现日志覆盖（index 连续）

---

## 二、故障场景测试

### 4. Follower 短暂离线后恢复
**测试目标**：验证日志追赶（Log Catch-up）机制
- 启动 3 节点集群
- 关闭 1 个 Follower
- Leader 写入 50 条日志
- 重启该 Follower
- 验证：
    - Follower 能自动追赶上所有日志
    - 最终 3 个节点的日志完全一致
    - 状态机数据一致

### 5. Follower 长期离线后恢复
**测试目标**：验证大量日志追赶
- 启动 5 节点集群
- 关闭 2 个 Follower
- Leader 写入 1000 条日志
- 重启这 2 个 Follower
- 验证：
    - Follower 能逐步追赶上所有日志
    - 追赶过程中不影响新写入
    - 最终所有节点一致

### 6. Leader 崩溃恢复测试
**测试目标**：已提交的日志不会丢失
- 启动 3 节点集群
- 写入 10 条日志，等待提交
- 杀死 Leader
- 验证：
    - 新 Leader 包含这 10 条日志
    - 新 Leader 继续写入，旧日志不丢失
    - 重启旧 Leader 后，它能正确同步日志

### 7. Leader 崩溃前部分复制
**测试目标**：未提交的日志可能被覆盖（Raft 安全性）
- 启动 3 节点集群（A, B, C）
- 网络隔离：Leader A 只能与 B 通信
- A 写入 5 条日志，只复制到 B（未提交）
- 杀死 A
- C 成为新 Leader，写入新日志
- 恢复 A，B
- 验证：
    - A 和 B 的未提交日志被覆盖
    - 最终所有节点与 C 的日志一致
    - commitIndex 以下的日志在所有节点一致

---

## 三、日志一致性测试

### 8. prevLogIndex/prevLogTerm 检查
**测试目标**：验证日志一致性检查机制
- 启动 3 节点集群
- 模拟场景：
    - Follower A 的日志：[1, 1, 1]（term, index）
    - Leader 尝试追加 (term=2, index=5)
- 验证：
    - Follower 拒绝追加（prevLogIndex=4 不存在）
    - Leader 递减 nextIndex 重试
    - 最终日志同步成功

### 9. 日志冲突解决
**测试目标**：不同任期的日志冲突处理
- 启动 5 节点集群
- 模拟网络分区导致的日志分叉：
    - 分区 1（2 节点）：term=1 写入 10 条日志
    - 分区 2（3 节点）：term=2 写入 5 条日志
- 恢复网络
- 验证：
    - term=1 的节点日志被 term=2 的覆盖
    - 最终所有节点日志一致
    - term=2 的日志得以保留

### 10. nextIndex 递减机制测试
**测试目标**：验证 Leader 能找到 Follower 的分叉点
- 启动 3 节点集群
- 人为构造 Follower 日志落后且有冲突
- 观察日志：
    - Leader 从 nextIndex = lastIndex 开始尝试
    - 失败后递减 nextIndex
    - 最终找到匹配点并同步

---

## 四、边界条件测试

### 11. 空日志集群启动
**测试目标**：初始状态正确性
- 启动 3 节点集群（全新，无持久化数据）
- 验证：
    - 所有节点 lastIndex = 0 或 -1
    - commitIndex = 0
    - 能正常写入第一条日志

### 12. 只有 Leader 有日志
**测试目标**：Leader 能向空日志 Follower 复制
- 启动 1 个节点，写入 10 条日志
- 启动另外 2 个新节点加入集群
- 验证：
    - 新节点能从头开始复制所有日志
    - 最终数据一致

### 13. 单节点集群（边界）
**测试目标**：单节点模式下的日志处理
- 启动 1 节点集群
- 写入数据
- 验证：
    - 单节点自动成为 Leader
    - 写入无需复制，直接提交
    - 日志和状态机正确更新

---

## 六、正确性验证测试

### 18. Linearizability 测试
**测试目标**：验证线性一致性
- 启动 3 节点集群
- 执行序列：
    - 客户端 1：PUT(x, 1)
    - 客户端 2：PUT(x, 2)
    - 客户端 3：GET(x)
- 验证：
    - GET 必须返回 2（最新写入）
    - 不能读到旧值或中间状态


---

## 七、持久化相关测试

### 21. 日志持久化验证
**测试目标**：日志能正确持久化和恢复
- 启动 1 个节点，写入 50 条日志
- 关闭节点（正常关闭）
- 重启节点
- 验证：
    - lastIndex 恢复正确
    - 所有日志都能读取
    - 状态机数据完整

### 22. 崩溃后日志恢复
**测试目标**：异常退出后的数据完整性
- 启动 3 节点集群
- 写入过程中强制杀死进程（kill -9）
- 重启
- 验证：
    - 已提交的日志不丢失
    - 未提交的日志可能丢失（但不影响一致性）
    - RocksDB 数据完整


---

## 测试建议

### 优先级排序
1. **P0（必须通过）**：1-7, 11, 18
2. **P1（重要）**：8-10, 12-14, 21-22


### 自动化建议
- 使用 JUnit 参数化测试批量执行
- 每个测试用例编写验证脚本（检查日志、状态机）
- 引入混沌工程工具（如 Jepsen）进行高级测试

### 调试建议
- 每个测试记录详细日志（term, index, commitIndex, lastApplied）
- 使用可视化工具展示日志复制过程
- 关键点设置断点验证变量状态

这 26 个测试用例覆盖了 Raft 日志复制的所有关键方面，建议先实现 P0 级别的测试！